{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Meta models investigation\n",
    "\n",
    "The goal of this notebook is to understand which architecture can learn things about neural networks (NN).\n",
    "Having NN as imput is a very different data type than images, text or tabular data, so we might need different processing layers to extract meaning from them.\n",
    "\n",
    "## Permutation of matrices\n",
    "In NN, one can permute a lot of the hidden activations (all activations in an MLP, channels in a CNN, ...) and still have the same network.\n",
    "\n",
    "The goal here is to train a NN with:\n",
    "- two matrices as input, $A$ and $B$\n",
    "- ouput is whether is a permutation of the rows and colums of the first, that is, if there are permutation matrices $P, Q$ such that $A = PBQ$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T15:26:00.468929778Z",
     "start_time": "2023-08-02T15:25:57.992635881Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Bool\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T16:09:37.904868277Z",
     "start_time": "2023-08-02T16:09:37.863284307Z"
    }
   },
   "outputs": [],
   "source": [
    "def permute_rows_and_cols(\n",
    "    matrix: Float[Tensor, \"*batch rows cols\"]\n",
    ") -> Float[Tensor, \"*batch rows cols\"]:\n",
    "    \"\"\"\n",
    "    Permutes randomly the rows and columns of the input matrix.\n",
    "    \"\"\"\n",
    "    *batch, rows, cols = matrix.shape\n",
    "    row_perm = torch.randperm(rows)\n",
    "    col_perm = torch.randperm(cols)\n",
    "    return matrix[..., row_perm, :][..., :, col_perm]\n",
    "\n",
    "\n",
    "def shuffle_matrix(\n",
    "    matrix: Float[Tensor, \"*batch rows cols\"]\n",
    ") -> Float[Tensor, \"*batch rows cols\"]:\n",
    "    \"\"\"\n",
    "    Shuffles all the elements of the matrix.\n",
    "    \"\"\"\n",
    "    *batch, rows, cols = matrix.shape\n",
    "    perm = torch.randperm(rows * cols)\n",
    "    matrix = matrix.view(*batch, -1)\n",
    "    return matrix[..., perm].view(*batch, rows, cols)\n",
    "\n",
    "\n",
    "def gen_training_data(\n",
    "    batch_size: int, mat_size: int, noise: float = 0.0\n",
    ") -> tuple[Float[Tensor, \"batch 2 rows cols\"], Bool[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Generates a batch of training data.\n",
    "    \"\"\"\n",
    "    A = torch.rand(batch_size, mat_size, mat_size)\n",
    "\n",
    "    permuted = permute_rows_and_cols(A)\n",
    "    shuffled = shuffle_matrix(A)\n",
    "\n",
    "    to_permute = torch.rand(batch_size) > 0.5\n",
    "    B = torch.where(to_permute.view(-1, 1, 1), permuted, shuffled)\n",
    "\n",
    "    if noise > 0:\n",
    "        A += torch.randn_like(A) * noise\n",
    "        B += torch.randn_like(B) * noise\n",
    "\n",
    "    x = torch.stack([A, B], dim=1)\n",
    "    return x, to_permute\n",
    "\n",
    "\n",
    "def get_accuracy(\n",
    "    model: nn.Module, mat_size: int, n_epochs: int = 1000, batch_size: int = 100\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model on the task of predicting if two matrices are permutations of each other.\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    for _ in range(n_epochs):\n",
    "        x, y = gen_training_data(batch_size, mat_size)\n",
    "        y_pred = model(x)\n",
    "        n_correct += ((y_pred > 0.5) == y.view(-1, 1)).sum().item()\n",
    "\n",
    "    return n_correct / (n_epochs * batch_size)\n",
    "\n",
    "\n",
    "def find_closed_row_col_perm(\n",
    "    matrix: Float[Tensor, \"row col\"], target: Float[Tensor, \"row col\"]\n",
    ") -> Float[Tensor, \"row col\"]:\n",
    "    \"\"\"\n",
    "    Finds the row and column permutations that minimize the distance between the matrix and the target.\n",
    "    \"\"\"\n",
    "\n",
    "    mat_size = matrix.shape[0]\n",
    "\n",
    "    best = None\n",
    "    dist_best = float(\"inf\")\n",
    "\n",
    "    for row_perm in itertools.permutations(range(mat_size)):\n",
    "        for col_perm in itertools.permutations(range(mat_size)):\n",
    "            permuted = matrix[row_perm, :][:, col_perm]\n",
    "            dist = ((permuted - target).abs()).sum()\n",
    "            if dist < dist_best:\n",
    "                best = permuted\n",
    "                dist_best = dist\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T15:26:03.551861397Z",
     "start_time": "2023-08-02T15:26:03.486622762Z"
    }
   },
   "outputs": [],
   "source": [
    "m = torch.arange(9).view(3, 3)\n",
    "print(m)\n",
    "print(permute_rows_and_cols(m))\n",
    "print(shuffle_matrix(m))\n",
    "print(gen_training_data(2, 3, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T10:22:17.474787209Z",
     "start_time": "2023-07-28T19:07:54.306198376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import AdamW\n",
    "import wandb\n",
    "\n",
    "\n",
    "MAT_SIZE = 3\n",
    "batch_size = 100\n",
    "n_epochs = 300_000\n",
    "lr = 1e-3\n",
    "use_wandb = True\n",
    "noise = 0.01\n",
    "hidden_multipliers = [4, 4]\n",
    "weight_decay = 1e-4\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "new_run = True\n",
    "if new_run:\n",
    "    model = make_mlp(hidden_multipliers, MAT_SIZE)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=\"meta-models\",\n",
    "            config={\n",
    "                \"batch_size\": batch_size,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"mat_size\": MAT_SIZE,\n",
    "                \"noise\": noise,\n",
    "                \"hidden_multipliers\": hidden_multipliers,\n",
    "                \"nb_hidden_layers\": len(hidden_multipliers),\n",
    "                \"nb_params\": sum(p.numel() for p in model.parameters()),\n",
    "            },\n",
    "        )\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = gen_training_data(batch_size, MAT_SIZE, noise=noise)\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y.view(-1, 1).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if use_wandb:\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    if epoch % 1_000 == 0:\n",
    "        accuracy = get_accuracy(model, MAT_SIZE, 20)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"accuracy\": accuracy})\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T11:05:26.900601983Z",
     "start_time": "2023-07-29T11:05:26.394586397Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(model, MAT_SIZE)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T11:10:41.452371142Z",
     "start_time": "2023-07-29T11:10:41.093066863Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "idx = len(list(models_dir.glob(\"*.pt\"))) + 1\n",
    "arch_str = \"_\".join(map(str, hidden_multipliers))\n",
    "name = f\"models/permutation-of-matrices-noisy-{MAT_SIZE}x{MAT_SIZE}-{accuracy*100:.0f}acc-{arch_str}arch-{idx}.pt\"\n",
    "\n",
    "torch.save(model, name)\n",
    "print(f\"Saved model to {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we compute the \"per permutation accuracy\", that is how frequently is the model correct when it predicts a permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T11:08:51.038268447Z",
     "start_time": "2023-07-29T11:08:50.528350994Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    row_perm: list[int]\n",
    "    col_perm: list[int]\n",
    "    correct: int\n",
    "    incorrect: int\n",
    "    predictions: list[bool]\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return self.correct / (self.correct + self.incorrect)\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "results = []\n",
    "for row_perm in tqdm(itertools.permutations(range(MAT_SIZE))):\n",
    "    for col_perm in itertools.permutations(range(MAT_SIZE)):\n",
    "        result = Result(row_perm, col_perm, 0, 0, [])\n",
    "        for _ in range(n_epochs):\n",
    "            x, y = gen_training_data(batch_size, MAT_SIZE)\n",
    "            x[..., 1, :, :] = x[..., 0, row_perm, :]\n",
    "            x[..., 1, :, :] = x[..., 1, :, col_perm]\n",
    "            y[...] = True\n",
    "            y_pred = model(x)\n",
    "            y = y.view(-1, 1)\n",
    "            correct = ((y_pred > 0.5) == y).sum().item()\n",
    "            result.correct += correct\n",
    "            result.incorrect += batch_size - correct\n",
    "            result.predictions.extend(y_pred.view(-1).tolist())\n",
    "        results.append(result)\n",
    "        print(f\"Accuracy for {row_perm} and {col_perm}: {result.accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T18:06:53.848385663Z",
     "start_time": "2023-07-28T18:06:53.430831064Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "nb_permutations = math.factorial(MAT_SIZE) ** 2\n",
    "nb_shuffles = math.factorial(MAT_SIZE**2)\n",
    "\n",
    "max_possible_accuracy = nb_permutations / nb_shuffles\n",
    "\n",
    "print(f\"Max possible accuracy: {max_possible_accuracy}\")\n",
    "print(f\"Nb permutations: {nb_permutations}\")\n",
    "print(f\"Nb shuffles: {nb_shuffles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the baseline from `find_closed_row_col_perm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T11:09:01.587671045Z",
     "start_time": "2023-07-29T11:09:00.950168056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find input on which the model is wrong\n",
    "x, y = gen_training_data(10000, MAT_SIZE)\n",
    "y_pred = model(x)\n",
    "wrong = (y_pred > 0.5).flatten() != y\n",
    "idx_wrong = wrong.nonzero(as_tuple=True)[0]\n",
    "x = x[idx_wrong]\n",
    "\n",
    "x: Tensor = torch.cat((x, torch.zeros_like(x)), dim=1)\n",
    "for a, b, c, d in x:\n",
    "    c[...] = find_closed_row_col_perm(a, b)\n",
    "    d[...] = b - c\n",
    "\n",
    "worst: Tensor = x[:, 3].flatten(1).abs().max(dim=1).values\n",
    "worst = worst.max(dim=0).indices.item()\n",
    "x = x[worst : worst + 1]\n",
    "\n",
    "px.imshow(\n",
    "    x.flatten(0, 1), facet_col=0, facet_col_wrap=4, height=400 * len(x), width=1200\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:35:16.834946496Z",
     "start_time": "2023-08-01T14:35:16.810720871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Layer that is invariant to permutations of rows and cols of a matrix\n",
    "\n",
    "# We can process stuff independently (invariant to permutations)\n",
    "# We can do equivariant mixing of values\n",
    "# We can then combine everything with pooling (max, mean, sum, attention...)\n",
    "\n",
    "\n",
    "class SetPermutationInvariant(nn.Module):\n",
    "    \"\"\"A layer that is symmetric polynomial of the input.\"\"\"\n",
    "\n",
    "    def __init__(self, d_in: int, n_head: int = 4, d_out: int | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.n_head = n_head\n",
    "        self.d_out = d_out or d_in\n",
    "\n",
    "        self.read_head = nn.Linear(d_in, n_head)\n",
    "        self.write_head = nn.Linear(7 * self.n_head, self.d_out)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"*batch token d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch token d_model\"]:\n",
    "        x = self.read_head(x)\n",
    "\n",
    "        # All the possible reductions\n",
    "        r1 = x.mean(dim=-2)\n",
    "        r2 = x.max(dim=-2).values\n",
    "        r3 = x.min(dim=-2).values\n",
    "        r4 = x.norm(dim=-2)\n",
    "        r5 = x.var(dim=-2)\n",
    "        r6 = x.median(dim=-2).values\n",
    "        # Kurtosis is not implemented in PyTorch\n",
    "        r7 = (x - r1.unsqueeze(-2)).pow(4).mean(dim=-2) / r5.pow(2)\n",
    "\n",
    "        reductions = torch.stack([r1, r2, r3, r4, r5, r6, r7], dim=-2)\n",
    "        return self.write_head(reductions.flatten(-2, -1))\n",
    "\n",
    "\n",
    "from metamodels.utils import MLP\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, in_dim: int, d_model: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.invariant_layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, d_model),\n",
    "            MLP(d_model, 4),\n",
    "        )\n",
    "        self.reduction = SetPermutationInvariant(d_model, d_out=7)\n",
    "        self.final = nn.Sequential(\n",
    "            MLP(7, 4, out_dim=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"*batch d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch d_model\"]:\n",
    "        x = self.invariant_layers(x)\n",
    "        x = self.reduction(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "a_set = torch.rand(2, 10, 1)\n",
    "n = NN(1, 12)\n",
    "print(n)\n",
    "pred = n(a_set)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:05:55.529230021Z",
     "start_time": "2023-08-01T16:05:55.466434150Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_permutation(x: Float[Tensor, \"batch vec_size_x_2\"]) -> Float[Tensor, \"batch\"]:\n",
    "    \"\"\"Return how far the second part of the vector is to a permutation of the first part.\"\"\"\n",
    "    *batch, vec_size = x.shape\n",
    "    assert vec_size % 2 == 0\n",
    "    vec_size //= 2\n",
    "    first = x[..., :vec_size]\n",
    "    second = x[..., vec_size:]\n",
    "    # Sort them both\n",
    "    first = first.sort(dim=-1).values\n",
    "    second = second.sort(dim=-1).values\n",
    "    # Compute the distance\n",
    "    return (first - second).abs().sum(dim=-1)\n",
    "\n",
    "\n",
    "def gen_training_data(\n",
    "    batch_size: int,\n",
    "    vec_size: int,\n",
    "    quantisation: float = 0.05,\n",
    "    add_last_dim: bool = False,\n",
    ") -> tuple[Float[Tensor, \"batch token\"], Bool[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Generates a batch of training data. All the elements of a batch use the same permutation.\n",
    "    \"\"\"\n",
    "    A = torch.randn(batch_size, vec_size)\n",
    "\n",
    "    permuted = A[:, torch.randperm(vec_size)]\n",
    "    completely_random = torch.rand_like(A)\n",
    "\n",
    "    to_permute = torch.rand(batch_size) > 0.5\n",
    "    B = torch.where(to_permute.view(-1, 1), permuted, completely_random)\n",
    "\n",
    "    # Quantize A and B\n",
    "    A = (A / quantisation).round() * quantisation\n",
    "    B = (B / quantisation).round() * quantisation\n",
    "\n",
    "    # Concatenate them\n",
    "    x = torch.cat((A, B), dim=-1)\n",
    "\n",
    "    # Compute correct labels (the random ones might be a permutation of the first ones)\n",
    "    y = is_permutation(x)\n",
    "\n",
    "    if add_last_dim:\n",
    "        x = x[..., None]\n",
    "    return x, y < quantisation\n",
    "\n",
    "\n",
    "vec_size = 20\n",
    "x, y = gen_training_data(2, vec_size, 0.1)\n",
    "print(x.shape, y.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "n(x[..., None])\n",
    "\n",
    "n2 = nn.Sequential(MLP(vec_size * 2, 4, out_dim=1), nn.Sigmoid())\n",
    "n2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:13:02.921111637Z",
     "start_time": "2023-08-01T16:13:02.895734200Z"
    }
   },
   "outputs": [],
   "source": [
    "# net = NN(1, 12)\n",
    "# train(net, lambda batch_size: gen_training_data(batch_size, 5, add_last_dim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:13:04.386183276Z",
     "start_time": "2023-08-01T16:13:04.338251249Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(net: nn.Module, data_generator) -> float:\n",
    "    correct = 0\n",
    "    for _ in range(100):\n",
    "        x, y = data_generator(100)\n",
    "        y_pred = net(x)\n",
    "        correct += ((y_pred > 0.5) == y.view(-1, 1)).sum().item()\n",
    "    return correct / (100 * 100)\n",
    "\n",
    "\n",
    "get_accuracy(net, lambda bs: gen_training_data(bs, vec_size, add_last_dim=last_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:13:06.689200648Z",
     "start_time": "2023-08-01T16:13:06.615411291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find adversarial examples on which the model is wrong\n",
    "\n",
    "\n",
    "def adv_examples(\n",
    "    net: nn.Module,\n",
    "    batch_size: int = 1,\n",
    "    steps: int = 100,\n",
    "    vec_size: int = 5,\n",
    "    add_last_dim: bool = False,\n",
    ") -> tuple[Float[Tensor, \"batch vec_size_x_2\"], Bool[Tensor, \"batch\"]]:\n",
    "    x = torch.rand(batch_size, vec_size * 2)\n",
    "    if add_last_dim:\n",
    "        x = x[..., None]\n",
    "    x.requires_grad = True\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        y_pred = net(x)\n",
    "        dist_to_perm = is_permutation(x.squeeze(-1))\n",
    "        y_true = dist_to_perm < 0.1\n",
    "        loss = loss_fn(y_pred, 1 - y_true.view(-1, 1).float())\n",
    "        # Add a term to maximize the distance to a permutation\n",
    "        # loss = loss + dist_to_perm.mean()\n",
    "        # Add a term to minimize the norm of the input\n",
    "        # loss = loss + x.norm(dim=1).mean() * 0\n",
    "        # Penalise values outside of [-1, 1]\n",
    "        loss = loss + (x.abs() - 1).clamp(min=0).mean() * 0.1\n",
    "        loss.backward()\n",
    "        x.data += x.grad.data * 0.01\n",
    "        x.grad = None\n",
    "        # Clamp the values\n",
    "        # x.data = torch.clamp(x.data, -1, 1)\n",
    "    if random.random() < 0.01:\n",
    "        print(\"Dist to perm:\", dist_to_perm.mean().item())\n",
    "        print(\"Loss:\", loss.item())\n",
    "    return x.detach(), y_true\n",
    "\n",
    "\n",
    "x, y = adv_examples(net, vec_size=vec_size, steps=100, add_last_dim=last_dim)\n",
    "\n",
    "x1 = x[0, :vec_size]\n",
    "x2 = x[0, vec_size:]\n",
    "x1_sorted = x1.sort().values\n",
    "x2_sorted = x2.sort().values\n",
    "\n",
    "px.imshow(torch.stack((x1_sorted, x2_sorted), dim=0), height=400, width=1200).show()\n",
    "\n",
    "# Check accuracy\n",
    "y_pred = net(x)\n",
    "print(\"* Pred:\", y_pred.item())\n",
    "print(\"* True:\", y.item())\n",
    "print(x.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:13:09.461370409Z",
     "start_time": "2023-08-01T16:13:09.438108644Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_size = 4\n",
    "net = NN(1, 12)\n",
    "net = nn.Sequential(MLP(vec_size * 2, 4, 1, out_dim=1), nn.Sigmoid())\n",
    "last_dim = False\n",
    "# net = nn.Sequential(SetPermutationInvariant(1, 2, 14), MLP(14, 1, out_dim=1), nn.Sigmoid())\n",
    "# last_dim = True\n",
    "# train(net, lambda batch_size: gen_training_data(batch_size, 5, add_last_dim=last_dim), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:17:42.203582759Z",
     "start_time": "2023-08-01T16:16:57.842826180Z"
    }
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    net,\n",
    "    lambda batch_size: gen_training_data(batch_size, vec_size, add_last_dim=last_dim),\n",
    "    adversary=lambda batch_size: adv_examples(\n",
    "        net, batch_size, vec_size=vec_size, add_last_dim=last_dim\n",
    "    ),\n",
    "    lr=1e-4,\n",
    "    steps=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:17:55.235275941Z",
     "start_time": "2023-08-01T16:17:55.171374996Z"
    }
   },
   "outputs": [],
   "source": [
    "get_accuracy(net, lambda bs: gen_training_data(bs, vec_size, add_last_dim=last_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:04:56.970814899Z",
     "start_time": "2023-08-01T16:04:56.936351545Z"
    }
   },
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:17:57.911321941Z",
     "start_time": "2023-08-01T16:17:57.836334642Z"
    }
   },
   "outputs": [],
   "source": [
    "w = net[0].layers[0].weight.detach().clone()\n",
    "b = net[0].layers[0].bias.detach().clone()\n",
    "w = torch.cat((w, b.unsqueeze(-1)), dim=-1)\n",
    "# w[w.abs() < 0.1] = float('nan')\n",
    "px.imshow(w, height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trying the dot product layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T16:18:40.631802517Z",
     "start_time": "2023-08-02T16:18:40.015557818Z"
    }
   },
   "outputs": [],
   "source": [
    "import metamodels.utils as utils\n",
    "import torchinfo\n",
    "\n",
    "N = 4\n",
    "free_space = N * 2\n",
    "d_model = N**2 * 2 + free_space\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    utils.SkipSequential(\n",
    "        utils.MLP(d_model, 4),\n",
    "        utils.DotProductLayer(d_model, 8, N),\n",
    "        utils.MLP(d_model, 4),\n",
    "        utils.DotProductLayer(d_model, 8, N),\n",
    "    ),\n",
    "    # Pooling layer\n",
    "    utils.MLP(d_model, 4, out_dim=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "torchinfo.summary(net, (100, 2, N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T15:30:12.735338763Z",
     "start_time": "2023-08-02T15:30:11.703926509Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.train(\n",
    "    net,\n",
    "    lambda bs: gen_training_data(bs, N),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
