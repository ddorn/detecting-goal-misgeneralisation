{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69677c152f22a9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:53:09.831216702Z",
     "start_time": "2023-08-14T16:53:09.802237638Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:53:12.395163346Z",
     "start_time": "2023-08-14T16:53:10.056606041Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from torch import nn\n",
    "from torchinfo import torchinfo\n",
    "\n",
    "import src as M\n",
    "import wandb\n",
    "\n",
    "# For plotly, to have larger images\n",
    "BIG = dict(width=1600, height=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a966f3a89b3b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:53:12.412162235Z",
     "start_time": "2023-08-14T16:53:12.396447124Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_SIZE = 4\n",
    "\n",
    "mk_env_generator = lambda full_color: M.wrap(\n",
    "    lambda: M.ThreeGoalsEnv(ENV_SIZE, step_reward=0.0),\n",
    "    # lambda e: M.ColorBlindWrapper(e, reduction='max', reward_indistinguishable_goals=True, disabled=full_color),\n",
    "    lambda e: M.OneHotColorBlindWrapper(e, reward_indistinguishable_goals=True, disabled=full_color),\n",
    "    lambda e: M.AddTrueGoalToObsFlat(e),\n",
    "    # lambda e: M.AddSwitch(e, 1, lambda _: 0),  # No switch, but we still use SwitchMLP as the architecture...\n",
    ")\n",
    "mk_env = mk_env_generator(full_color=False)\n",
    "mk_env_full_color = mk_env_generator(full_color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b443ae4baaa9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:53:12.663870747Z",
     "start_time": "2023-08-14T16:53:12.412935937Z"
    }
   },
   "outputs": [],
   "source": [
    "red, green, blue = M.ThreeGoalsEnv.GOAL_CELLS\n",
    "# env = mk_env_full_color()\n",
    "env = mk_env()\n",
    "\n",
    "print(env)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "check_env(env)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "if isinstance(obs, dict):\n",
    "    switch = obs['switch']\n",
    "    print(f\"{switch=}\")\n",
    "    obs = obs['obs']\n",
    "    \n",
    "print(f\"{obs.shape=}\")\n",
    "px.imshow(obs[:-\n",
    "3].reshape(4, 4, 3)).show()\n",
    "print(obs[-3:])\n",
    "# print(obs[:80].astype(int).reshape(4, 4, -1))\n",
    "# print(obs[80:].astype(int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db6b9034e4a09b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:48:55.987945286Z",
     "start_time": "2023-08-14T19:48:55.371988792Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "arch = M.Split(-3,\n",
    "   left= nn.Sequential(\n",
    "       M.Rearrange(\"... (h w c) -> ... c h w\", h=ENV_SIZE, w=ENV_SIZE, c=5),\n",
    "       nn.Conv2d(5, 8, 3, padding=1),\n",
    "       nn.ReLU(),\n",
    "       nn.Conv2d(8, 8, 3, padding=1),\n",
    "       nn.ReLU(),\n",
    "       # nn.Conv2d(16, 4, 3, padding=1),\n",
    "       # nn.ReLU(),\n",
    "       nn.Flatten(-3),\n",
    "   ),\n",
    "   right=nn.Identity(),\n",
    ")\n",
    "\n",
    "arch = nn.Sequential(\n",
    "    arch,\n",
    "    # A.Rearrange(\"... h w c -> ... c h w\", h=ENV_SIZE, w=ENV_SIZE, c=3),\n",
    "    # nn.Conv2d(3, 8, 3, padding=1),\n",
    "    # nn.ReLU(),\n",
    "    # nn.Conv2d(8, 8, 3, padding=1),\n",
    "    # nn.ReLU(),\n",
    "    nn.LazyLinear(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    # nn.Linear(32, 32),\n",
    "    # nn.ReLU(),\n",
    "    # nn.Linear(32, 32),\n",
    "    # nn.ReLU(),\n",
    "    # M.MLP(4 * ENV_SIZE ** 2 + 3, 32, 32, add_act_after=True),\n",
    ")\n",
    "\n",
    "print(torchinfo.summary(arch, input_size=(7, *obs.shape), depth=4))\n",
    "print(arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466be5caadc1e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:55:32.593216564Z",
     "start_time": "2023-08-14T19:49:00.766119769Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "n_env = 4\n",
    "use_wandb = True\n",
    "l1_weight_decay = 8e-4\n",
    "seed = randint(0, 2**32 - 1)\n",
    "\n",
    "assert not isinstance(arch, M.L1WeightDecay), \"You forgot to re-run the arch definition\"\n",
    "arch = M.L1WeightDecay(arch, l1_weight_decay)\n",
    "\n",
    "def lr_schedule(f):\n",
    "    # Ugly hack to also change the weight decay\n",
    "    wd = (1-f) * l1_weight_decay\n",
    "    try:\n",
    "        policy.policy.mlp_extractor.policy_net.weight_decay = wd\n",
    "        policy.logger.record(\"train/l1_weight_decay\", wd)\n",
    "    except (NameError, AttributeError):\n",
    "        pass  # Initial call before policy is defined\n",
    "    return 1e-3 * f\n",
    "\n",
    "policy = PPO(\n",
    "    M.CustomActorCriticPolicy,\n",
    "    make_vec_env(mk_env, n_envs=n_env),\n",
    "    policy_kwargs=dict(\n",
    "        arch=arch,\n",
    "        optimizer_kwargs=dict(weight_decay=0),\n",
    "    ),\n",
    "    n_steps=2_048 // n_env,\n",
    "    tensorboard_log=\"../run_logs\",\n",
    "    seed=seed,\n",
    "    device='cpu',\n",
    ")\n",
    "policy.lr_schedule = lr_schedule\n",
    "print(policy.policy)\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in policy.policy.parameters()))\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        save_code=True,\n",
    "        config=dict(\n",
    "            l1_weight_decay=l1_weight_decay,\n",
    "            arch=str(arch),\n",
    "            seed=seed,\n",
    "        ),\n",
    "        project=\"3goals-blind\",\n",
    "        notes=M.unique(\"\"\"\n",
    "First try with a CNN policy (RGB images - RG colorbind) (+fix): 60% acc :/\n",
    "Trying now with 3x3 convs with padding, so that the size of the image doesn't change: 60% acc :/\n",
    "Again with a relu at the end: 60% acc :/\n",
    "Now lower regularization: (1e-4 -> 1e-5):  60% acc :/\n",
    "AHHH I the images were shaped wrong! Added an MLP before the conv. Second try...: 60% acc :/\n",
    "Turned L1 to 0. Lr 3e-4 -> 1e-5: still bad\n",
    "Added one more conv layer, lr back to default.\n",
    "Removed the convs. Tried with full color. Bump lr to 1e-3\n",
    "Just a simple MLP. Fuck.\n",
    "Now using just the 4x4 RandomGoalEnv with no colorblindness (fix)\n",
    "Learns in 100k steps with lr=1e-3 * f\n",
    "Now using a CNN. It works. Now colorblind.\n",
    "Now using 3GoalEnv, with no true goal indicator and blindness (it should go to red/green and not blue): it does 66% correct goal \n",
    "Now using Split, to process the image and the true goal separately. Then merge and MLP it: 68% acc 30% wrong goal\n",
    "\n",
    "Setting step reward to 0: It works! in 300k steps, 97% acc.\n",
    "Adding L1 regularisation 1e-4: 97% acc\n",
    "Using now 1-hot encoding: 98% acc in 500k steps. Pattern is not very clear.\n",
    "Increase L1 to 1e-3. Too strong.\n",
    "510: Now 3e-4. It worked well, was a bit more often towards the green than red. \n",
    "511: Bump again to 4e-4. Was apparently too strong, it did not learn much to go to the blue goal.\n",
    "512: Trying to increase l1 over time: (1-f) * 3e-4. Fail: I launched the same run as before\n",
    "513: Same as above, but for real. 44%red vs 50%green\n",
    "514: Same run. Same results since everything is seeded :facepalm:\n",
    "515: Changed seed -> Slight red preference\n",
    "516: Changed seed -> Slight green pref\n",
    "517: Changed seed -> red\n",
    "519: Changed seed -> red\n",
    "520: Bumped l1 to 8e-4 * (1-f) ->\n",
    "\"\"\"))\n",
    "\n",
    "callbacks = [M.ProgressBarCallback()]\n",
    "if use_wandb:\n",
    "    callbacks.append(M.WandbWithBehaviorCallback(mk_env()))\n",
    "policy.learn(total_timesteps=400_000, callback=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5a0f6d83e0a64",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-14T16:58:10.710745518Z"
    }
   },
   "outputs": [],
   "source": [
    "# policy.lr_schedule = lambda _: 2e-4\n",
    "policy.policy.mlp_extractor.policy_net.weight_decay = 3e-4\n",
    "policy.learn(total_timesteps=300_000, reset_num_timesteps=False, callback=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa4ef19630a801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:56:30.598794784Z",
     "start_time": "2023-08-14T19:56:12.676501091Z"
    }
   },
   "outputs": [],
   "source": [
    "M.make_stats(policy, mk_env(), n_episodes=10_000, subtitle=\"Blind agent with blind inputs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6900cfa006dbe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:56:49.029599272Z",
     "start_time": "2023-08-14T19:56:30.598120303Z"
    }
   },
   "outputs": [],
   "source": [
    "M.make_stats(policy, mk_env_full_color(),\n",
    "             n_episodes=10_000,\n",
    "             subtitle=\"Blind agent with full color inputs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2f446e733e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 0\n",
    "M.evaluate(policy, mk_env_generator(color)(), n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c422eb1efa3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "M.show_behavior(policy, M.ThreeGoalsEnv.interesting(4, 10, [mk_env]), **BIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73867436b9ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = []\n",
    "for i in range(6):\n",
    "    base_env = M.ThreeGoalsEnv.constant(ENV_SIZE, true_goal={\"red\": 1, \"green\": 1})\n",
    "    envs.append(mk_env(base_env))\n",
    "    envs.append(mk_env_full_color(base_env))\n",
    "        \n",
    "M.show_behavior(policy, envs, **BIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257d5ad74724bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:57:08.768635778Z",
     "start_time": "2023-08-14T19:57:08.396353651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print all models\n",
    "models_dir = Path(\"models\")\n",
    "for model_path in sorted(models_dir.glob(\"*.zip\")):\n",
    "    print(model_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbadfeb4c036e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T19:57:34.323381309Z",
     "start_time": "2023-08-14T19:57:33.913023197Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"blind-4x4-model-cnn-pref-41-56green\"\n",
    "path = models_dir / f\"{name}.zip\"\n",
    "if path.exists():\n",
    "    print(\"Loading existing model\")\n",
    "    try:\n",
    "        previous_policy = policy  # Saved, in case I wanted to save it, but forgot to change the name\n",
    "    except NameError: \n",
    "        pass\n",
    "    policy = PPO.load(path)\n",
    "else:\n",
    "    print(f\"Saving model to {path}\")\n",
    "    try:\n",
    "        policy.save(path)\n",
    "    except TypeError:\n",
    "        # Without this I get TypeError: can't pickle LazyModule objects\n",
    "        # I don't know why, but my hack for the weight_decay seem to interfere with\n",
    "        # their saving mechanism\n",
    "        policy.lr_schedule = lambda _: -1\n",
    "        policy.learning_rate = -1\n",
    "        policy.save(path)\n",
    "    else:\n",
    "        path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804cbe0a143c097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:45:07.043558623Z",
     "start_time": "2023-08-14T16:45:06.718497830Z"
    }
   },
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import base64\n",
    "from stable_baselines3.common.save_util import save_to_zip_file, is_json_serializable\n",
    "\n",
    "\n",
    "def save(self, path, exclude = None, include = None) -> None:\n",
    "    \"\"\"\n",
    "    Save all the attributes of the object and the model parameters in a zip-file.\n",
    "\n",
    "    :param path: path to the file where the rl agent should be saved\n",
    "    :param exclude: name of parameters that should be excluded in addition to the default ones\n",
    "    :param include: name of parameters that might be excluded but should be included anyway\n",
    "    \"\"\"\n",
    "    # Copy parameter list so we don't mutate the original dict\n",
    "    data = self.__dict__.copy()\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    # Exclude is union of specified parameters (if any) and standard exclusions\n",
    "    if exclude is None:\n",
    "        exclude = []\n",
    "    exclude = set(exclude).union(self._excluded_save_params())\n",
    "\n",
    "    # Do not exclude params if they are specifically included\n",
    "    if include is not None:\n",
    "        exclude = exclude.difference(include)\n",
    "\n",
    "    state_dicts_names, torch_variable_names = self._get_torch_save_params()\n",
    "    all_pytorch_variables = state_dicts_names + torch_variable_names\n",
    "    for torch_var in all_pytorch_variables:\n",
    "        # We need to get only the name of the top most module as we'll remove that\n",
    "        var_name = torch_var.split(\".\")[0]\n",
    "        # Any params that are in the save vars must not be saved by data\n",
    "        exclude.add(var_name)\n",
    "\n",
    "    # Remove parameter entries of parameters which are to be excluded\n",
    "    for param_name in exclude:\n",
    "        data.pop(param_name, None)\n",
    "\n",
    "    # Build dict of state_dicts\n",
    "    params_to_save = self.get_parameters()\n",
    "    \n",
    "    for data_key, data_item in data.items():\n",
    "        # See if object is JSON serializable\n",
    "        if is_json_serializable(data_item):\n",
    "            # All good, store as it is\n",
    "            pass\n",
    "        else:\n",
    "            # Not serializable, cloudpickle it into\n",
    "            # bytes and convert to base64 string for storing.\n",
    "            # Also store type of the class for consumption\n",
    "            # from other languages/humans, so we have an\n",
    "            # idea what was being stored.\n",
    "            print(data_key, type(data_item), data_item)\n",
    "            base64_encoded = base64.b64encode(cloudpickle.dumps(data_item)).decode()\n",
    "\n",
    "    save_to_zip_file(path, data=data, params=params_to_save)\n",
    "    \n",
    "policy.learning_rate = 2e-4\n",
    "policy.lr_schedule\n",
    "\n",
    "save(policy, \"osef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847ca7280381dfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T15:33:23.811347117Z",
     "start_time": "2023-08-14T15:33:23.747339420Z"
    }
   },
   "outputs": [],
   "source": [
    "names = dir()\n",
    "for name in names:\n",
    "    if name[:2] == '_i' and name[2:].isnumeric():\n",
    "        del globals()[name]\n",
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae57831fa740ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4133102ea0ef53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T17:26:40.330014245Z",
     "start_time": "2023-08-11T17:26:40.267149060Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(x, symetric: bool = True, **kwargs):\n",
    "    h, w = x.shape[-2:]\n",
    "    if 'facet_col' in kwargs:\n",
    "        wrap = kwargs.get('facet_col_wrap', 1)\n",
    "        h *= np.ceil(x.shape[kwargs['facet_col']] / wrap)\n",
    "        w *= wrap\n",
    "    if symetric:\n",
    "        kwargs.setdefault(\"color_continuous_midpoint\", 0)\n",
    "        kwargs.setdefault(\"color_continuous_scale\", \"RdBu\")\n",
    "    width = 50 + w * 25\n",
    "    height = h * 25 + 50 * ('title' in kwargs)\n",
    "    while width < 500 and height < 500:\n",
    "        width *= 2\n",
    "        height *= 2\n",
    "    new = dict(\n",
    "        width=max(300, width),\n",
    "        height=max(300, height),\n",
    "        facet_row_spacing=0.01,\n",
    "        facet_col_spacing=0.01,\n",
    "    )\n",
    "    kwargs = {**new, **kwargs}\n",
    "    px.imshow(x, **kwargs).show()\n",
    "\n",
    "\n",
    "switch_biases = policy.policy.mlp_extractor.switch_biases\n",
    "print(\"Biases shape:\", switch_biases.shape)\n",
    "print(\"Max abs bias:\", switch_biases.abs().max(dim=-1).values)\n",
    "imshow(switch_biases, title='Biases of the switch layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f1569b1d07b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the three switch layers\n",
    "\n",
    "switch_layers_weights = policy.policy.mlp_extractor.switch_weights\n",
    "\n",
    "# shape of a switch (out_dim, row, col, obj_type)\n",
    "w1 = einops.rearrange(switch_layers_weights, 'agent out_dim (row col obj_type) -> agent obj_type out_dim row col', row=4, col=4)\n",
    "b1 = einops.repeat(policy.policy.mlp_extractor.switch_biases, 'agent out_dim -> agent 1 out_dim 4 1')\n",
    "\n",
    "avg = w1.mean(dim=0)\n",
    "TYPES = ['empty', 'agent', 'goal_red', 'goal_green', 'goal_blue']\n",
    "EMPTY, AGENT, RED, GREEN, BLUE = range(5)\n",
    "\n",
    "print(w1.shape, avg.shape)\n",
    "# d = w1[:, EMPTY] - avg[EMPTY] \n",
    "d = avg\n",
    "\n",
    "d = w1[..., :-3]\n",
    "print(d.shape)\n",
    "# Add one black col\n",
    "d = torch.cat([d, torch.zeros(*d.shape[:-1], 1) + float('nan')], dim=-1)\n",
    "d = torch.cat([d, torch.zeros(*d.shape[:-2], 1, d.shape[-1]) + float('nan')], dim=-2)\n",
    "d = einops.rearrange(d, 'agent obj out row col -> out (agent row) (obj col)')[..., :-1, :]\n",
    "b1 = torch.cat([b1, torch.zeros(*b1.shape[:-2], 1, b1.shape[-1]) + float('nan')], dim=-2)\n",
    "b1 = einops.rearrange(b1, 'agent obj out row col -> out (agent row) (obj col)')[..., :-1, :]\n",
    "print(d.shape, b1.shape)\n",
    "d = torch.cat([d, b1], dim=-1)\n",
    "# Remove weights close to zero\n",
    "# d[abs(d) < 0.1] = float('nan')\n",
    "imshow(d[:16], title='First layer weights', facet_col=0, facet_col_wrap=4,\n",
    "          # height=4000,\n",
    "          # width=None,\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451389ecd3d25bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(w1.flatten(2).mean(dim=2),\n",
    "          title=\"Mean of the weights of the switch layers\",\n",
    "          labels=dict(x=\"Object type\", y=\"Agent\"),\n",
    "       symetric=False,\n",
    "          )\n",
    "imshow(w1.flatten(2).abs().mean(dim=2), \n",
    "          title=\"Mean absolute value of the weights of the switch layers\",\n",
    "          labels=dict(x=\"Object type\", y=\"Agent\"),\n",
    "       symetric=False,\n",
    "          )\n",
    "imshow(w1.flatten(2).std(dim=2),\n",
    "            title=\"Std of the weights of the switch layers\",\n",
    "            labels=dict(x=\"Object type\", y=\"Agent\"),\n",
    "       symetric=False,\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675396154733477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer: torch.nn.Linear = policy.policy.action_net\n",
    "last_weights = last_layer.weight.detach().cpu().clone()\n",
    "last_bias = last_layer.bias.detach().cpu().clone()\n",
    "net = policy.policy.mlp_extractor.policy_net.module\n",
    "\n",
    "weights = torch.cat([\n",
    "    last_weights.T @ net.switches[i].weight.detach().cpu().clone()\n",
    "    for i in range(3)\n",
    "], dim=0)\n",
    "imshow(weights)\n",
    "\n",
    "biases = torch.stack([net.switches[i].bias.detach().cpu() for i in range(3)], dim=1)\n",
    "imshow(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b34be96b254156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlations between rows of w2\n",
    "w2 = net.post_switch[1].weight.detach().cpu().clone()  # (64, 64)\n",
    "imshow(w2)\n",
    "w2 = w2 / w2.norm(dim=1, keepdim=True)\n",
    "corr = w2 @ w2.T\n",
    "\n",
    "# Cluster the correlations matrix\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial.distance as ssd\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Compute and plot first dendrogram.\n",
    "fig = ff.create_dendrogram(\n",
    "    corr.numpy(),\n",
    "    orientation='left',\n",
    "    labels=list(range(64)),\n",
    "    linkagefun=lambda x: sch.linkage(x, 'single'),\n",
    "    distfun=lambda x: ssd.pdist(x, 'euclidean'),\n",
    ")\n",
    "fig.update_layout(width=1000, height=1000)\n",
    "fig.show()\n",
    "\n",
    "# Remove the diagonal\n",
    "corr[range(64), range(64)] = float('nan')\n",
    "px.imshow(corr, width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d24673e959d72d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Looking at the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd6cf7362bd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs: list[M.ThreeGoalsEnv]\n",
    "n = 5000\n",
    "envs = [add_wrappers(M.ThreeGoalsEnv.constant(), disabled=False) for _ in range(n)]\n",
    "inputs = [env.reset()[0] for env in envs]\n",
    "with M.record_activations(policy.policy) as cache:\n",
    "    for i in inputs:\n",
    "        policy.predict(i)\n",
    "        \n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55c213b058b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.stack([obs['obs'] for obs in inputs])\n",
    "switches = np.stack([obs['switch'] for obs in inputs])\n",
    "print(\"Observations shape:\", observations.shape)\n",
    "print(\"Switches shape:\", switches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7feb120e25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_positions = torch.tensor([e.goal_positions for e in envs])\n",
    "agent_positions = torch.tensor([e.agent_pos for e in envs])\n",
    "red_goals = goal_positions[:, 0]\n",
    "green_goals = goal_positions[:, 1]\n",
    "blue_goals = goal_positions[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417f1c39b6b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation between each act and red_goal.x\n",
    "def one_hot_encode(arr, maxi=4):\n",
    "    # Set up one-hot encoding array\n",
    "    one_hot = np.zeros((*arr.shape, maxi))\n",
    "    # Populate the one-hot encoding array        \n",
    "    np.put_along_axis(one_hot, arr[..., None], 1, axis=-1)\n",
    "    return one_hot\n",
    "\n",
    "# to_check = goal_positions.flatten(1)\n",
    "# to_check = one_hot_encode(to_check)\n",
    "to_check = np.concatenate([\n",
    "    goal_positions.flatten(1),\n",
    "    agent_positions], \n",
    "    axis=-1)\n",
    "\n",
    "for name in (\"switches.0\", \"switches.1\"):\n",
    "    act = cache[name]\n",
    "    corrs = np.corrcoef(act, to_check, rowvar=False)\n",
    "    # remove the diagonal\n",
    "    corrs[range(corrs.shape[0]), range(corrs.shape[0])] = 0\n",
    "    imshow(corrs[32:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3100b8c45e5d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_red_blind = np.corrcoef(cache['switches.0'], goal_positions[:, 0], rowvar=False)\n",
    "corr_blue_color = np.corrcoef(cache['switches.1'], goal_positions[:, 2], rowvar=False)\n",
    "corr_red_blind = corr_red_blind[32:, :-2]\n",
    "corr_blue_color = corr_blue_color[32:, :-2]\n",
    "corrs = einops.rearrange([corr_red_blind, corr_blue_color], \n",
    "                         \"type dim neuron -> (dim type) neuron\")\n",
    "\n",
    "imshow(corrs)\n",
    "imshow(corr_red_blind)\n",
    "imshow(corr_blue_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15fad411bb6137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T16:32:51.128355846Z",
     "start_time": "2023-08-14T16:32:50.703333441Z"
    }
   },
   "outputs": [],
   "source": [
    "conv1 = policy.policy.mlp_extractor.policy_net.module[0].left[1]\n",
    "conv1 = conv1.weight.detach().numpy()\n",
    "conv1 = einops.repeat(conv1, \"out in row col -> out row (in col)\")\n",
    "px.imshow(conv1, facet_col=0, \n",
    "          color_continuous_midpoint=0,\n",
    "          color_continuous_scale=\"RdBu\", facet_col_wrap=4)\n",
    "# M.L1WeightDecay(arch, 0.01)\n",
    "\n",
    "\n",
    "# conv1 = arch.module[0].left[3].weight.detach()\n",
    "# conv1\n",
    "# conv1 = einops.repeat(conv1, \"out in row col -> (out in) (row col)\")\n",
    "# print(conv1.shape)\n",
    "# px.imshow(conv1, **BIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8edb2631856175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
